# Multi-token-attention
Unofficial paper implementation of "Multi Token Attention" by Olga et al published by FAIR at Meta. 

# Implementation Roadmap 

- [x] MTA Pytorch implementation 
- [ ] Evaluation on pretraining comparing with other attention mechanisms 
- [ ] Evaluation on fine-tuning comparing with other attention mechanisms
- [ ] MTA CUDA implementation with custom cuda kernel for convolution to accurately get causal mask for MTA.